{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "a9ec1791_f8951493",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 10,
      "author": {
        "id": 1290458
      },
      "writtenOn": "2022-10-17T19:31:49Z",
      "side": 1,
      "message": "Can you clarify the bug here and how it relates to b/213617178? We spent a lot of time auditing this code last year. It\u0027s quite complex but we came to the conclusion it\u0027s safe. In particular signalfd claims to compress multiple SIGCHILDs so it\u0027s necessary to reap all outstanding children.\n\nWe did want to simplify this code and replace it with pidfd this year, but I think that\u0027s unlikely to happen right now.",
      "revId": "31997af63cbfb8eba1fa960a8896ac4c3e82e643",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "0d2c081a_7e9875b3",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 10,
      "author": {
        "id": 1064128
      },
      "writtenOn": "2022-10-17T20:10:30Z",
      "side": 1,
      "message": "+1. I saw this CL when looking at the epoll CLs. We\u0027ve experimented with this in the past and it always resulted in missing processes to reap. As David said, we couldn\u0027t reliable get either signalfd or the original signal handlers to consistently trigger per-process.",
      "parentUuid": "a9ec1791_f8951493",
      "revId": "31997af63cbfb8eba1fa960a8896ac4c3e82e643",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "9ffdc4c3_44f047c7",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 10,
      "author": {
        "id": 1869780
      },
      "writtenOn": "2022-10-19T14:17:02Z",
      "side": 1,
      "message": "Regarding the question of how this CL is related to b/213617178: the presubmit run for https://android-review.googlesource.com/c/platform/system/core/+/2218645 on September 27 reported a large boot time regression. This CL makes that boot time regression disappear.\n\nRegarding reaping all child processes that exited: isn\u0027t that already guaranteed by calling ReapAnyOutstandingChildren() after epoll_wait() returned?\n\nTom, no Unix kernel guarantees that one SIGCHLD handler call happens per child process that exited. Unix kernels use a single bit to track the \"pending\" state of signals like SIGCHLD (see also the POSIX specification - https://pubs.opengroup.org/onlinepubs/9699919799/functions/V2_chap02.html#tag_15_04). It can happen that multiple child processes exit after SIGCHLD became pending and before the SIGCHLD signal handler is activated.\n\nIs more information available about reliability issues that have been encountered in the past with signalfd?",
      "parentUuid": "0d2c081a_7e9875b3",
      "revId": "31997af63cbfb8eba1fa960a8896ac4c3e82e643",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "bd342071_64f3c2e8",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 10,
      "author": {
        "id": 1064128
      },
      "writtenOn": "2022-10-20T04:34:38Z",
      "side": 1,
      "message": "\u003e ...on September 27 reported a large boot time regression. This CL makes that boot time regression disappear.\n\nDoes that regression reproduce or do you have more details on why this CL fixes the regression? I don\u0027t see anything in those CLs that would cause a boot time regression that this CL would fix.\n\n\u003e Regarding reaping all child processes that exited: isn\u0027t that already guaranteed by calling ReapAnyOutstandingChildren() after epoll_wait() returned?\n\nThat sounds right though it\u0027s been a while since I\u0027ve looked at this code. What is the motivation behind this CL if we\u0027ll eventually call ReapAnyOutstandingChildren() to read other child processes? I don\u0027t understand why it\u0027s wrong to disregard the SIGCHLD information.\n\n\u003e Tom, no Unix kernel guarantees that one SIGCHLD handler call happens per child process that exited. Unix kernels use a single bit to track the \"pending\" state of signals like SIGCHLD (see also the POSIX specification -\n\nRight, maybe we only tried with the signalfd.\n\n\u003e Is more information available about reliability issues that have been encountered in the past with signalfd?\n\nI don\u0027t have any off-hand. Maybe in old CLs or bugs of mine there\u0027s more information, though the code has changed a lot since then so it likely isn\u0027t worth finding. The summary is we tried removing calls to `ReapAnyOutstandingChildren()` and strictly relying on the SIGCHLD information from the signal FD and then init failed to reap all children. We didn\u0027t see a reason to investigation significantly further since this isn\u0027t code that needs to be highly performance and is code that needs to be highly stable, so marginal extra overhead to call ReapAnyOutstandingChildren() was decided to be worth it.",
      "parentUuid": "9ffdc4c3_44f047c7",
      "revId": "31997af63cbfb8eba1fa960a8896ac4c3e82e643",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "5152ea0e_7746b140",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 10,
      "author": {
        "id": 1869780
      },
      "writtenOn": "2022-10-20T16:42:26Z",
      "side": 1,
      "message": "Patch \"Migrate the blkio controller to the v2 cgroup hierarchy\" (https://android-review.googlesource.com/c/platform/system/core/+/2218645) introduces an additional step in cgroup configuration, namely activation of the blkio controller. This happens by the parent process and the child process waits for completion of that step. I\u0027m not sure of this but my guess is that this change makes it more likely for the read((*pipefd)[0], \u0026byte, 1) call in Service::RunService() to sleep. This in turn makes it more likely that kill() is called from the parent process before setpgid() has been called by the child process.",
      "parentUuid": "bd342071_64f3c2e8",
      "revId": "31997af63cbfb8eba1fa960a8896ac4c3e82e643",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "d0bb60a1_39b9e922",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 10,
      "author": {
        "id": 1064128
      },
      "writtenOn": "2022-10-20T16:54:48Z",
      "side": 1,
      "message": "In that case, it would be better to fix that CL instead of modifying very fundamental and very brittle process reaping code.\n\nCould the setpgid() be moved to before waiting on the pipe? A quick look makes me think that is possible.",
      "parentUuid": "5152ea0e_7746b140",
      "revId": "31997af63cbfb8eba1fa960a8896ac4c3e82e643",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "044d8766_2f3ae953",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 10,
      "author": {
        "id": 1869780
      },
      "writtenOn": "2022-10-20T17:29:18Z",
      "side": 1,
      "message": "Do you want me to call setpgid() from the parent process or from the child process? There is a mechanism in the child process to wait for the parent process but there is not yet a mechanism in the parent process to wait for the child process. Do you perhaps want me to add a mechanism in the parent process to wait for the child process? If so, do you want me to measure how much this makes Service::Start() slower?",
      "parentUuid": "d0bb60a1_39b9e922",
      "revId": "31997af63cbfb8eba1fa960a8896ac4c3e82e643",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "051cd15a_d116e507",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 10,
      "author": {
        "id": 1064128
      },
      "writtenOn": "2022-10-20T22:28:53Z",
      "side": 1,
      "message": "I meant to move setpgid() before waiting on the pipe, to avoid the latency of waiting on the pipe which seems to make the race condition easier to hit. Moving it before that read would alleviate the race and processes would work similarly to how they did before. Admittedly, it wouldn\u0027t completely solve the race.\n\nCalling setpgid() in the parent would be tricky in two cases. 1) For services that have a console setsid() is called instead of setpgid() [1] and setsid() fails if the process is already a process group leader. With pid namespaces, fork is called twice [2] and the grand-child process of init needs to be the process group leader.\n\n1: https://cs.android.com/android/platform/superproject/+/master:system/core/init/service_utils.cpp;l\u003d244?q\u003df:init%20f:service%20setpgid\n2: https://cs.android.com/android/platform/superproject/+/master:system/core/init/service_utils.cpp;l\u003d100;drc\u003db45a2ea782074944f79fc388df20b06e01f265f7?q\u003dclone\u0026ss\u003dandroid%2Fplatform%2Fsuperproject:system%2Fcore%2Finit%2F\n\nHaving the parent wait for the child is risky and time sensitive. It\u0027s risky since if the child never signals the parent then init may wait forever. It\u0027s time sensitive since a majority of services are started via the core and main classes. In those cases Service::Start() is called in a loop and it\u0027s ideal that we fork the child processes quickly. \n\nStepping back:\n1) If there is added delay in launching services due to the cgroup change, that should be addressed given the above time sensitivity. It may be best to have more fundamental changes to how these cgroups are set up. Maybe they\u0027re done by the child and init has a new state for services (\u0027initializing\u0027) that transitions to running only after they\u0027ve been set up?\n2) The cgroup issue with kill and process group leaders could be solved with the freezing cgroup work. I don\u0027t know the status of that, but it seems reasonable to reach out and understand that team\u0027s progress.",
      "parentUuid": "044d8766_2f3ae953",
      "revId": "31997af63cbfb8eba1fa960a8896ac4c3e82e643",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    }
  ]
}