{
  "comments": [
    {
      "key": {
        "uuid": "eae3eda8_761e12ef",
        "filename": "init/tokenizer_test.cpp",
        "patchSetId": 3
      },
      "lineNbr": 78,
      "author": {
        "id": 1003224
      },
      "writtenOn": "2018-06-27T20:34:48Z",
      "side": 1,
      "message": "(if we cared enough to rewrite this, keeping track of the current line number should probably just move into the lexer.)",
      "revId": "2a5a4e72f3410e14e41fc691bbd36698b50cd6e1",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": true
    }
  ]
}