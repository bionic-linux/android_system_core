{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "b105666d_077054aa",
        "filename": "fs_mgr/libsnapshot/snapuserd/user-space-merge/snapuserd_core.cpp",
        "patchSetId": 5
      },
      "lineNbr": 183,
      "author": {
        "id": 1045980
      },
      "writtenOn": "2021-10-08T23:31:50Z",
      "side": 1,
      "message": "Is there a particular reason we add 1 here, vs just checking \u003d\u003d 0?",
      "range": {
        "startLine": 183,
        "startChar": 26,
        "endLine": 183,
        "endChar": 29
      },
      "revId": "2072c2fca183142ce571d34a1feff3ec5f1043b2",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c6738a9a_a1d2e5b4",
        "filename": "fs_mgr/libsnapshot/snapuserd/user-space-merge/snapuserd_transitions.cpp",
        "patchSetId": 5
      },
      "lineNbr": 415,
      "author": {
        "id": 1045980
      },
      "writtenOn": "2021-10-08T23:31:50Z",
      "side": 1,
      "message": "Why do we need to wait here? The blocks we need to access are always available. Can\u0027t we just get them from the readahead buffer here? We\u0027d need to hold up the readahead thread from making forwards progress until those I/Os are completed, but a reader/writer style lock for the read ahead buffer should cover that.\n\nIt may be possible to do the same as in MERGE_PENDING, if the source is not getting overwritten, but it\u0027s almost certainly better to go with what\u0027s in the readahead cache, since that\u0027s already there.",
      "range": {
        "startLine": 412,
        "startChar": 6,
        "endLine": 415,
        "endChar": 64
      },
      "revId": "2072c2fca183142ce571d34a1feff3ec5f1043b2",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "9d826a16_f12bbbce",
        "filename": "fs_mgr/libsnapshot/snapuserd/user-space-merge/snapuserd_transitions.cpp",
        "patchSetId": 5
      },
      "lineNbr": 415,
      "author": {
        "id": 1724998
      },
      "writtenOn": "2021-10-09T06:36:21Z",
      "side": 1,
      "message": "Yes, we could get them from readahead buffer but that will hold the read-ahead thread to make forward progress if we have high volume I/O\u0027s coming from root filesystem; read-ahead thread will have to wait until all I/O\u0027s are completed as we cannot wipe the buffer when there are in-flight I/O\u0027s being served from the buffer. If read-ahead thread doesn\u0027t make forward progress, then merge thread also will be held up.\n\nAs I mentioned above, the I/O wait here is extremely minimal and is bound only for one window of the merge. Now the only concern here is what if some application just keeps pumping I/O\u0027s on one region - then as I mentioned above, merge will be held up. This is unlikely as we have never seen this till date.\n\nLet me think through if I can address the merge starvation issue.",
      "parentUuid": "c6738a9a_a1d2e5b4",
      "range": {
        "startLine": 412,
        "startChar": 6,
        "endLine": 415,
        "endChar": 64
      },
      "revId": "2072c2fca183142ce571d34a1feff3ec5f1043b2",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "4f3a08d5_094a6e3c",
        "filename": "fs_mgr/libsnapshot/snapuserd/user-space-merge/snapuserd_transitions.cpp",
        "patchSetId": 5
      },
      "lineNbr": 456,
      "author": {
        "id": 1045980
      },
      "writtenOn": "2021-10-08T23:49:33Z",
      "side": 1,
      "message": "Potential starvation issue if we have a high volume of accesses. This is probably what we want to have minimal user impact, just wanted to mention that, especially if we get reports of slow application time.",
      "range": {
        "startLine": 453,
        "startChar": 8,
        "endLine": 456,
        "endChar": 9
      },
      "revId": "2072c2fca183142ce571d34a1feff3ec5f1043b2",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b47744b1_ebcee275",
        "filename": "fs_mgr/libsnapshot/snapuserd/user-space-merge/snapuserd_transitions.cpp",
        "patchSetId": 5
      },
      "lineNbr": 456,
      "author": {
        "id": 1724998
      },
      "writtenOn": "2021-10-09T06:36:21Z",
      "side": 1,
      "message": "I did think through this when working on this design. The starvation here is not on the application side. All the I/O from dm-user a.k.a root filesystem will be served immediately. However, it will wait only during the merge window. Now, this cannot be a starvation as the merge thread will _always_ make forward progress - The wait here is only for a specific window of merge which is 510 blocks and not on entire merge completion. Thus this wait should be less than 500ms.\n\nNow, the starvation can happen the other way around wherein we have high volume I/O from application continuously and merge thread could potentially wait indefinitely. However, this is the same design we have in the kernel snapshots as well. It is ok for merge thread to wait as that will not have impact on the application. However, if we want to address this starvation, we need some flow control mechanics and make sure both make forward progress.",
      "parentUuid": "4f3a08d5_094a6e3c",
      "range": {
        "startLine": 453,
        "startChar": 8,
        "endLine": 456,
        "endChar": 9
      },
      "revId": "2072c2fca183142ce571d34a1feff3ec5f1043b2",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2aba5421_a347ebb7",
        "filename": "fs_mgr/libsnapshot/snapuserd/user-space-merge/snapuserd_transitions.cpp",
        "patchSetId": 5
      },
      "lineNbr": 506,
      "author": {
        "id": 1045980
      },
      "writtenOn": "2021-10-08T20:07:02Z",
      "side": 1,
      "message": "This name doesn\u0027t indicate that the function may block. By the name, I would expect it to simply return the state, but it actually attempts set up for I/O.\n\nCan we rename it, perhaps something like RequestIOStart? Not sure if that\u0027s the best name, but this is a companion to NotifyIOCompletion.",
      "range": {
        "startLine": 506,
        "startChar": 35,
        "endLine": 506,
        "endChar": 53
      },
      "revId": "2072c2fca183142ce571d34a1feff3ec5f1043b2",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    }
  ]
}