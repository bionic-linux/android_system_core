{
  "comments": [
    {
      "key": {
        "uuid": "2b17e08c_c53e5a02",
        "filename": "liblog/logd_write.c",
        "patchSetId": 2
      },
      "lineNbr": 222,
      "author": {
        "id": 1005146
      },
      "writtenOn": "2015-02-16T14:44:06Z",
      "side": 1,
      "message": "Only logging this into the events stream eliminates something we found valuable.  Our perspective is that of somebody analyzing the content of the logs.  If I see log message A in thread X, then later see log message C in thread X, and I expected to see log message B in thread X in between, but didn\u0027t, I want to see the \"Dropped X\" messages for thread X, so that I can distinguish between a log drop and an actual bug.",
      "range": {
        "startLine": 222,
        "startChar": 20,
        "endLine": 222,
        "endChar": 33
      },
      "revId": "2353fa6a306f74f599e162a46616bc3378124278",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "2b17e08c_08c887ac",
        "filename": "liblog/logd_write.c",
        "patchSetId": 2
      },
      "lineNbr": 222,
      "author": {
        "id": 1005146
      },
      "writtenOn": "2015-02-16T14:46:03Z",
      "side": 1,
      "message": "Hmm, I see your comments about using \u0027logcat -d -b main -b events\u0027 to do the interleaving.  The problem for us is that we typically use logs from our persistent reader collected passively by other users, and the streams aren\u0027t interleaved.",
      "parentUuid": "2b17e08c_c53e5a02",
      "range": {
        "startLine": 222,
        "startChar": 20,
        "endLine": 222,
        "endChar": 33
      },
      "revId": "2353fa6a306f74f599e162a46616bc3378124278",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "8885328c_406d8e93",
        "filename": "liblog/logd_write.c",
        "patchSetId": 2
      },
      "lineNbr": 222,
      "author": {
        "id": 1032276
      },
      "writtenOn": "2015-02-23T16:03:51Z",
      "side": 1,
      "message": "Interleaving is somewhat specific to your re-logging infrastructure. I will look into an adjustment to logd/logcat (as hinted above) that will help address your concerns over interleaving.\n\nYour logging infrastructure breaks our security concerns over PII by storing the data in persistent files. What really needs to be done is that the bugreport mechanism needs to be adjusted to address your needs. Please advise on the shortcomings you have with bugreports so we may fix it to work for you.",
      "parentUuid": "2b17e08c_08c887ac",
      "range": {
        "startLine": 222,
        "startChar": 20,
        "endLine": 222,
        "endChar": 33
      },
      "revId": "2353fa6a306f74f599e162a46616bc3378124278",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "a880367a_01654a7f",
        "filename": "liblog/logd_write.c",
        "patchSetId": 2
      },
      "lineNbr": 222,
      "author": {
        "id": 1032276
      },
      "writtenOn": "2015-02-23T16:03:51Z",
      "side": 1,
      "message": "I understand. Please be advised that your reaction is a result of FUD associated with a _bug_ that you had thankfully resolved in logd; hopefully never to bite us again.\n\nI am not comfortable with creating a statistical infrastructure to record TID and LogID for each associated log drop, then play them back. The goal is to re-establish trust, I am sorry I broke that trust. But I still need/want an inexpensive mechanism to report logging failures if they occur in order to take corrective action on the platform. This message is to be considered \u0027fatal\u0027 from the logging perspective and I need to make that more prominent to the observer is my take (I will think about that). My gut tell me that the event needs to be special-cased in logd or logcat ...\n\nAs for being able to continue to triage a problem in the face of data-loss, almost all scenarios one can comes up with will not benefit triage with a more specific loss statistic. The reaction should be to fix the platform problem (hopefully one line in init.rc, and not fix a bug in the logger) and stop limping along.\n\nNB: putting 10000 into /proc/sys/net/unix/dgram_max_qlen and restarting logd can be done without adjusting the platform init.rc file. That should have _bypassed_ the bug in logd that was fixed by your CL.",
      "parentUuid": "2b17e08c_c53e5a02",
      "range": {
        "startLine": 222,
        "startChar": 20,
        "endLine": 222,
        "endChar": 33
      },
      "revId": "2353fa6a306f74f599e162a46616bc3378124278",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "6b447857_af749932",
        "filename": "liblog/logd_write.c",
        "patchSetId": 2
      },
      "lineNbr": 240,
      "author": {
        "id": 1032276
      },
      "writtenOn": "2015-02-13T00:39:47Z",
      "side": 1,
      "message": "\u0027dropped\u0027 can get damaged by multiple threads feeding log messages at the same time. We can not afford to use locking as it adds considerable overhead. Explore means to locklessly(sic) improve the reliability of the \u0027dropped\u0027 value.\n\nWhat does not get lost in the shuffle is that this event tells us messages were dropped, regardless if the number is correct or not. This piece of information is more valuable than the quantity.",
      "range": {
        "startLine": 219,
        "startChar": 0,
        "endLine": 240,
        "endChar": 5
      },
      "revId": "2353fa6a306f74f599e162a46616bc3378124278",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "ab3af0c2_d45e5230",
        "filename": "liblog/logd_write.c",
        "patchSetId": 2
      },
      "lineNbr": 240,
      "author": {
        "id": 1001401
      },
      "writtenOn": "2015-02-13T00:57:26Z",
      "side": 1,
      "message": "Can we use an atomic set here and and atomic inc below?",
      "parentUuid": "6b447857_af749932",
      "range": {
        "startLine": 219,
        "startChar": 0,
        "endLine": 240,
        "endChar": 5
      },
      "revId": "2353fa6a306f74f599e162a46616bc3378124278",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "cb5e442b_d0c9b45e",
        "filename": "liblog/logd_write.c",
        "patchSetId": 2
      },
      "lineNbr": 240,
      "author": {
        "id": 1032276
      },
      "writtenOn": "2015-02-13T02:52:56Z",
      "side": 1,
      "message": "I was testing it already ... but decided to use atomic_add and inc",
      "parentUuid": "ab3af0c2_d45e5230",
      "range": {
        "startLine": 219,
        "startChar": 0,
        "endLine": 240,
        "endChar": 5
      },
      "revId": "2353fa6a306f74f599e162a46616bc3378124278",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "6b2558ee_67905236",
        "filename": "liblog/logd_write.c",
        "patchSetId": 2
      },
      "lineNbr": 240,
      "author": {
        "id": 1005146
      },
      "writtenOn": "2015-02-16T14:44:06Z",
      "side": 1,
      "message": "My original proposal used __thread to avoid this, which is a GNU extension.  It looks like C++11 has a standard mechanism (thread_local), but I didn\u0027t try it.  I guess if you\u0027re using atomic on a normal static then you don\u0027t care about thread-specific counts.",
      "parentUuid": "cb5e442b_d0c9b45e",
      "range": {
        "startLine": 219,
        "startChar": 0,
        "endLine": 240,
        "endChar": 5
      },
      "revId": "2353fa6a306f74f599e162a46616bc3378124278",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "6b2558ee_a7a9aa6c",
        "filename": "liblog/logd_write.c",
        "patchSetId": 2
      },
      "lineNbr": 240,
      "author": {
        "id": 1005146
      },
      "writtenOn": "2015-02-16T14:52:38Z",
      "side": 1,
      "message": "This says nothing about performance impacts, though.  If I recall correctly from our rough profiling, we were looking at a penalty of about 80 ns on one of our low-tier devices (Moto E).  At the time, we figured it was worth it for at least our dev builds.",
      "parentUuid": "6b2558ee_67905236",
      "range": {
        "startLine": 219,
        "startChar": 0,
        "endLine": 240,
        "endChar": 5
      },
      "revId": "2353fa6a306f74f599e162a46616bc3378124278",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "2b2600b6_d07782b6",
        "filename": "liblog/logd_write.c",
        "patchSetId": 2
      },
      "lineNbr": 310,
      "author": {
        "id": 1032276
      },
      "writtenOn": "2015-02-13T00:39:47Z",
      "side": 1,
      "message": "We should limit to INT32_MAX/",
      "range": {
        "startLine": 310,
        "startChar": 8,
        "endLine": 310,
        "endChar": 18
      },
      "revId": "2353fa6a306f74f599e162a46616bc3378124278",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "2b17e08c_e541d67c",
        "filename": "liblog/logd_write.c",
        "patchSetId": 2
      },
      "lineNbr": 310,
      "author": {
        "id": 1005146
      },
      "writtenOn": "2015-02-16T14:44:06Z",
      "side": 1,
      "message": "We went with only 8 bits, since we figured past 255 drops the precision just wasn\u0027t interesting anymore.  Our perspective is that of somebody analyzing the log content, though, not profiling the system for logging throughput.",
      "parentUuid": "2b2600b6_d07782b6",
      "range": {
        "startLine": 310,
        "startChar": 8,
        "endLine": 310,
        "endChar": 18
      },
      "revId": "2353fa6a306f74f599e162a46616bc3378124278",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    }
  ]
}